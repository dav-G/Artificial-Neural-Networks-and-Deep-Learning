{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Model"]},{"cell_type":"markdown","metadata":{"id":"ars66CFYXclP"},"source":["## Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:44.684513Z","iopub.status.busy":"2023-11-12T22:24:44.683747Z","iopub.status.idle":"2023-11-12T22:24:44.888927Z","shell.execute_reply":"2023-11-12T22:24:44.887906Z","shell.execute_reply.started":"2023-11-12T22:24:44.684478Z"},"id":"r9KWlV35XgF7","trusted":true},"outputs":[],"source":["# Fix randomness and hide warnings\n","SEED = 42\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n","os.environ['PYTHONHASHSEED'] = str(SEED)\n","os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n","\n","import warnings\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","warnings.simplefilter(action='ignore', category=Warning)\n","\n","import numpy as np\n","np.random.seed(SEED)\n","\n","import logging\n","\n","import random\n","random.seed(SEED)\n","\n","# Import tensorflow\n","import tensorflow as tf\n","from tensorflow import keras as tfk\n","from tensorflow.keras import layers as tfkl\n","tf.autograph.set_verbosity(0)\n","tf.get_logger().setLevel(logging.ERROR)\n","tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n","tf.random.set_seed(SEED)\n","tf.compat.v1.set_random_seed(SEED)\n","\n","# Import other libraries\n","import matplotlib.pyplot as plt\n","from keras.applications.convnext import ConvNeXtLarge, preprocess_input as convnext_l_preprocess\n","from keras.applications.convnext import ConvNeXtXLarge, preprocess_input as convnext_xl_preprocess\n","from keras.applications.efficientnet_v2 import EfficientNetV2L, preprocess_input as efficientnet_preprocess\n","from tensorflow.keras.applications.resnet_v2 import preprocess_input as inceptionresnet_preprocess\n","from tensorflow.keras.applications.xception import Xception, preprocess_input as xception_preprocess\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.utils import class_weight, shuffle\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n","import seaborn as sns\n","from keras.preprocessing.image import ImageDataGenerator\n","from tqdm import tqdm\n","from math import ceil\n","import keras_tuner as kt\n","import keras_cv\n","import os\n","import shutil"]},{"cell_type":"markdown","metadata":{},"source":["## Set Platform"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def setPlatform(platform):\n","    if platform == \"Colab\":\n","        from google.colab import drive\n","        drive.mount('/gdrive')\n","        %cd /gdrive/My Drive/AN2DL challenges/challenge1\n","        return \"dataset_clean.npz\", \"logfiles/\"\n","    elif platform == \"Kaggle\":\n","        try:\n","            # Output directory cleaning\n","            shutil.rmtree(\"/kaggle/working\")\n","        except:\n","            print(\"Output directory is empty.\")\n","        return \"/kaggle/input/dataset/dataset_clean.npz\", \"/kaggle/working/logfiles/\"\n","    else:\n","        raise Exception(\"Error: platform should be Colab or Kaggle.\")\n","\n","# platform : \"Colab\" | \"Kaggle\"\n","PLATFORM = \"Kaggle\"\n","DATASET_PATH, LOGFILE_PATH = setPlatform(PLATFORM)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:44.891167Z","iopub.status.busy":"2023-11-12T22:24:44.890829Z","iopub.status.idle":"2023-11-12T22:24:44.900202Z","shell.execute_reply":"2023-11-12T22:24:44.899307Z","shell.execute_reply.started":"2023-11-12T22:24:44.891140Z"},"trusted":true},"outputs":[],"source":["if not os.path.exists(LOGFILE_PATH):\n","    os.makedirs(LOGFILE_PATH)\n","else:\n","    try:\n","        shutil.rmtree(LOGFILE_PATH)\n","        os.makedirs(LOGFILE_PATH)\n","    except OSError as e:\n","        f\"The folder {LOGFILE_PATH} does not exist.\""]},{"cell_type":"markdown","metadata":{"id":"84Ylj_UhYJq7"},"source":["## Load and process the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:44.901494Z","iopub.status.busy":"2023-11-12T22:24:44.901228Z","iopub.status.idle":"2023-11-12T22:24:45.546921Z","shell.execute_reply":"2023-11-12T22:24:45.545926Z","shell.execute_reply.started":"2023-11-12T22:24:44.901471Z"},"id":"wqBMh_mmtQ_U","outputId":"7c4c3fa1-75fc-495f-d62e-f658f84c72e8","trusted":true},"outputs":[],"source":["items = np.load(DATASET_PATH, allow_pickle=True)\n","leaves = items['data']\n","labels = items['labels']\n","\n","print(f'Input shape: {leaves.shape[1:]}\\n')\n","\n","# Calculate the unique target labels and their counts\n","unique, count = np.unique(labels, return_counts=True)\n","print('Target labels:', unique)\n","for u in unique:\n","    print(f'Class {unique[u]} has {count[u]} samples')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:45.549902Z","iopub.status.busy":"2023-11-12T22:24:45.549627Z","iopub.status.idle":"2023-11-12T22:24:45.877351Z","shell.execute_reply":"2023-11-12T22:24:45.876286Z","shell.execute_reply.started":"2023-11-12T22:24:45.549878Z"},"id":"2BPJxFtGwSoY","outputId":"f0801e63-e4ec-4572-f17b-6f9d78aa84b1","trusted":true},"outputs":[],"source":["# Convert labels to one-hot encoding format\n","labels = tfk.utils.to_categorical(labels, 2)\n","\n","# Random shuffle data\n","leaves, labels = shuffle(leaves, labels)\n","\n","# Split data into train_val and test sets\n","X_train_val, X_test, y_train_val, y_test = train_test_split (\n","    leaves,\n","    labels,\n","    random_state = SEED,\n","    test_size = 0.1,\n","    stratify = np.argmax(labels, axis=1)\n",")\n","\n","# Further split train_val into train and validation sets\n","X_train, X_val, y_train, y_val = train_test_split (\n","    X_train_val,\n","    y_train_val,\n","    random_state = SEED,\n","    test_size = len(X_test),\n","    stratify = np.argmax(y_train_val, axis=1)\n",")\n","\n","# Print shapes of the datasets\n","print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n","print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")\n","print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"5EItjnN9wtcZ"},"source":["## Training parameters"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:45.878812Z","iopub.status.busy":"2023-11-12T22:24:45.878528Z","iopub.status.idle":"2023-11-12T22:24:45.885283Z","shell.execute_reply":"2023-11-12T22:24:45.884294Z","shell.execute_reply.started":"2023-11-12T22:24:45.878787Z"},"id":"FQfNeLMJ5Bjm","trusted":true},"outputs":[],"source":["BATCH_SIZE = 128\n","EPOCHS = 1000\n","DROPOUT_RATE = 0.5\n","LR = 1e-4\n","\n","# EarlyStopping patience\n","ES_PATIENCE = 20\n","\n","# ReduceLROnPlateau patience\n","RLROP_PATIENCE = 15\n","\n","INPUT_SHAPE = X_train.shape[1:]\n","OUTPUT_SHAPE = y_train.shape[-1]\n","\n","callbacks = [\n","    tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=ES_PATIENCE, restore_best_weights=True),\n","    tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=RLROP_PATIENCE, min_lr=1e-6, mode='max')\n","]\n","\n","# keras applications supported: ConvNeXtLarge, ConvNeXtXLarge, EfficientNetV2L, Xception, InceptionResNetV2\n","KERAS_APP = \"ConvNeXtLarge\""]},{"cell_type":"markdown","metadata":{"id":"GRvxdEV8vG2b"},"source":["## Data Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:45.886703Z","iopub.status.busy":"2023-11-12T22:24:45.886407Z","iopub.status.idle":"2023-11-12T22:24:46.052939Z","shell.execute_reply":"2023-11-12T22:24:46.052101Z","shell.execute_reply.started":"2023-11-12T22:24:45.886678Z"},"id":"lVHqgPXwvG2b","trusted":true},"outputs":[],"source":["dataAugmentation = ImageDataGenerator(\n","    horizontal_flip = True,\n","    vertical_flip = True,\n","    rotation_range = 10,\n","    zoom_range = 0.1,\n","    fill_mode = \"nearest\"\n",")\n","\n","dataAugmentation.fit(X_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Data Augmentation for Imbalanced Classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def dataAugImbalanced(X_train, y_train):\n","    train_class_counts = [\n","        len(np.where(y_train[:, 0] == 1)[0]),\n","        len(np.where(y_train[:, 1] == 1)[0])\n","    ]\n","    min_class = np.argmin(train_class_counts)\n","    class_difference = np.max(train_class_counts) - train_class_counts[min_class]\n","\n","    print(f\"class_counts {train_class_counts}\\n\")\n","\n","    minority_class_indices = np.where(y_train[:, min_class] == 1)[0]\n","\n","    oversampled_X = np.empty((0,) + INPUT_SHAPE)\n","    oversampled_y = np.empty((0, OUTPUT_SHAPE))\n","\n","    idx = np.random.choice(minority_class_indices, class_difference, False)\n","    for i in idx:\n","        augmented_img = dataAugmentation.random_transform(X_train[i], seed=SEED)\n","        oversampled_X = np.vstack([oversampled_X, augmented_img[np.newaxis, :]])\n","        oversampled_y = np.vstack([oversampled_y, y_train[i]])\n","\n","    X_train_oversampled = np.vstack([X_train, oversampled_X])\n","    y_train_oversampled = np.vstack([y_train, oversampled_y])\n","\n","    print(f\"Classe 0: {len(np.where(y_train_oversampled[:, 0] == 1)[0])}\")\n","    print(f\"Classe 1: {len(np.where(y_train_oversampled[:, 1] == 1)[0])}\")\n","\n","    return X_train_oversampled, y_train_oversampled"]},{"cell_type":"markdown","metadata":{},"source":["## Class Weights for Imbalanced Classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def compute_class_weights(labels, y_train):\n","    class_weights = class_weight.compute_class_weight(\n","        class_weight = 'balanced',\n","        classes = np.unique(labels),\n","        y = np.argmax(y_train, axis=1)\n","    )\n","    class_weights = dict(enumerate(class_weights))\n","    return class_weights"]},{"cell_type":"markdown","metadata":{"id":"4nYu1qb9l_fH"},"source":["## Fine Tuning"]},{"cell_type":"markdown","metadata":{},"source":["### Create and return the base model with specified settings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def getBaseModel(preprocessing):\n","    if KERAS_APP == \"ConvNeXtLarge\":\n","        base_model = ConvNeXtLarge(\n","            include_top = False,\n","            weights = \"imagenet\",\n","            input_shape = INPUT_SHAPE,\n","            pooling = \"avg\",\n","            classifier_activation = \"softmax\"\n","        )\n","        return base_model(\n","            convnext_l_preprocess(preprocessing)\n","        )\n","    \n","    elif KERAS_APP == \"ConvNeXtXLarge\":\n","        base_model = ConvNeXtXLarge(\n","            include_top = False,\n","            weights = \"imagenet\",\n","            input_shape = INPUT_SHAPE,\n","            pooling = \"avg\",\n","            classifier_activation = \"softmax\"\n","        )\n","        return base_model(\n","            convnext_xl_preprocess(preprocessing)\n","        )\n","    \n","    elif KERAS_APP == \"EfficientNetV2L\":\n","        base_model = EfficientNetV2L(\n","            include_top = False,\n","            weights = \"imagenet\",\n","            input_shape = INPUT_SHAPE,\n","            pooling = \"avg\",\n","            classifier_activation = \"softmax\"\n","        )\n","        return base_model(\n","            efficientnet_preprocess(preprocessing)\n","        )\n","    \n","    elif KERAS_APP == \"Xception\":\n","        base_model = Xception(\n","            include_top = False,\n","            weights = \"imagenet\",\n","            input_shape = (299, 299, 3),\n","            pooling = \"avg\",\n","            classifier_activation = \"softmax\"\n","        )\n","        return base_model(\n","            xception_preprocess(\n","                tfkl.Resizing(\n","                    299, 299, interpolation='bicubic', name='resizing'\n","                )(preprocessing)\n","            )\n","        )\n","    \n","    elif KERAS_APP == \"InceptionResNetV2\":\n","        base_model = tfk.applications.InceptionResNetV2(\n","            include_top = False,\n","            weights = \"imagenet\",\n","            input_shape = (224, 224, 3),\n","            pooling = \"avg\",\n","            classifier_activation = \"softmax\"\n","        )\n","        return base_model(\n","            inceptionresnet_preprocess(\n","                tfkl.Resizing(\n","                    224, 224, interpolation='bicubic', name='resizing'\n","                )(preprocessing)\n","            )\n","        )\n","    \n","    else:\n","        raise Exception(\"Error: look for the keras applications supported.\")\n","\n","def getLayerToFreeze():\n","    if KERAS_APP == \"ConvNeXtLarge\": \n","        return \"convnext_large\"\n","    \n","    elif KERAS_APP == \"ConvNeXtXLarge\":\n","        return \"convnext_xlarge\"\n","    \n","    elif KERAS_APP == \"EfficientNetV2L\":\n","        return \"efficientnetv2-l\"\n","    \n","    elif KERAS_APP == \"Xception\":\n","        return \"xception\"\n","    \n","    elif KERAS_APP == \"InceptionResNetV2\":\n","        return \"inception_resnet_v2\"\n","        \n","    else:\n","        raise Exception(\"Error: look for the keras applications supported.\")\n","\n","def getPreprocessToUse():\n","    if KERAS_APP == \"ConvNeXtLarge\": \n","        return convnext_l_preprocess\n","    \n","    elif KERAS_APP == \"ConvNeXtXLarge\":\n","        return convnext_xl_preprocess\n","    \n","    elif KERAS_APP == \"EfficientNetV2L\":\n","        return efficientnet_preprocess\n","    \n","    elif KERAS_APP == \"Xception\":\n","        return xception_preprocess\n","    \n","    elif KERAS_APP == \"InceptionResNetV2\":\n","        return inceptionresnet_preprocess\n","        \n","    else:\n","        raise Exception(\"Error: look for the keras applications supported.\")"]},{"cell_type":"markdown","metadata":{},"source":["### Hypertuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_model_hypertuning(hp):\n","    tf.random.set_seed(SEED)\n","\n","    # Create a Model connecting input and output\n","    input_layer = tfk.Input(shape=INPUT_SHAPE)\n","    \n","    # best Random Flip\n","    hpRandomFlip = hp.Choice('RandomFlip', values=['horizontal_and_vertical', 'horizontal', 'vertical'])\n","    # best Random Rotation\n","    hpRandomRotation = hp.Float('RandomRotation', min_value=0.1, max_value=0.5, step=0.1)\n","    # best Random Zoom\n","    hpRandomZoom = hp.Float('RandomZoom', min_value=0.1, max_value=0.3, step=0.1)\n","    # best Random Translation\n","    hpRandomTranslation1 = hp.Float('RandomTranslation1', min_value=0.1, max_value=0.4, step=0.1)\n","    hpRandomTranslation2 = hp.Float('RandomTranslation2', min_value=0.1, max_value=0.4, step=0.1)    \n","\n","    # Image Augmentation\n","    preprocessing = tf.keras.Sequential(\n","        [\n","            tfkl.RandomFlip(hpRandomFlip),\n","            tfkl.RandomRotation(hpRandomRotation),\n","            tfkl.RandomZoom(hpRandomZoom),\n","            tfkl.RandomTranslation(hpRandomTranslation1, hpRandomTranslation2),        \n","        ],\n","        name='preprocessing'\n","    ) (input_layer)\n"," \n","    base_model = getBaseModel(preprocessing)\n","    \n","    # best number of units\n","    hp_units1 = hp.Choice('units-HiddenDense1', values=[128, 256, 512, 1024, 2048])\n","    hp_units2 = hp.Choice('units-HiddenDense2', values=[128, 256, 512, 1024, 2048])\n","    \n","    # best activation\n","    hp_activation1 = hp.Choice('activation1', values=['swish','elu','relu'])\n","    hp_activation2 = hp.Choice('activation-HiddenDense2', values=['swish','elu','relu'])\n","    \n","    \"\"\"\n","    # best dropout\n","    hp_dp1 = hp.Float('Dropout1', min_value=0.2, max_value=0.7, step=0.1)\n","    hp_dp2 = hp.Float('Dropout2', min_value=0.2, max_value=0.7, step=0.1)\n","    \"\"\"\n","\n","\n","    x = tfkl.Dense(\n","        units = hp_units1,\n","        kernel_initializer = tfk.initializers.HeUniform(seed=SEED),\n","        name = \"HiddenDense1\",\n","    )(base_model)\n","    # Batch Normalization\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.Activation(hp_activation1)(x)\n","    \n","    # Dropout\n","    # x = tfkl.Dropout(DROPOUT_RATE, seed=SEED, name=\"Dropout1\")(x)\n","    \n","    \n","    x = tfkl.Dense(\n","        units = hp_units2,\n","        kernel_initializer = tfk.initializers.HeUniform(seed=SEED),\n","        name = \"HiddenDense2\",\n","    )(x)\n","    # Batch Normalization\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.Activation(hp_activation2)(x) \n","\n","    # Dropout\n","    # x = tfkl.Dropout(DROPOUT_RATE, seed=SEED, name=\"Dropout2\")(x)\n","\n","    outputs = tfkl.Dense(\n","        2,\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed=SEED),\n","        activation = 'softmax'\n","    )(x)\n","    \n","    model = tfk.Model(input_layer, outputs, name='model')\n","        \n","    # Freeze the first N layers\n","    layers = model.get_layer(getLayerToFreeze()).layers\n","    N = ceil(len(layers) * 0.2)\n","    for _, layer in enumerate(layers[:N]):\n","        layer.trainable = False\n","    \n","    # best learning rate\n","    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4, 1e-5, 1e-6])\n","\n","    # Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n","    model.compile(\n","        loss = tfk.losses.CategoricalCrossentropy(),\n","        # Optimizer that implements the AdamW algorithm\n","        optimizer = tfk.optimizers.AdamW(learning_rate=hp_learning_rate),\n","        metrics = ['accuracy']\n","    )\n","    return model\n","\n","\n","def start_hypertuning(X_train, y_train, batch_size=BATCH_SIZE, callbacks=callbacks):\n","    tuner = kt.BayesianOptimization(\n","        build_model_hypertuning,\n","        objective = 'val_accuracy',\n","        max_trials = 10,\n","        directory = LOGFILE_PATH\n","    )\n","    \n","    tuner.search(\n","        X_train*255,\n","        y_train,\n","        validation_data = (\n","            X_val*255,\n","            y_val\n","        ),\n","        epochs=50,\n","        batch_size=batch_size,\n","        callbacks=callbacks\n","    )\n","    best_hp = tuner.get_best_hyperparameters()[0].values\n","    print(best_hp)\n","    return best_hp"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:46.054446Z","iopub.status.busy":"2023-11-12T22:24:46.054147Z","iopub.status.idle":"2023-11-12T22:24:46.067114Z","shell.execute_reply":"2023-11-12T22:24:46.066077Z","shell.execute_reply.started":"2023-11-12T22:24:46.054420Z"},"id":"JBY8ixn2vG2b","trusted":true},"outputs":[],"source":["def build_model():\n","    tf.random.set_seed(SEED)\n","\n","    # Create a Model connecting input and output\n","    input_layer = tfk.Input(shape=INPUT_SHAPE)\n","\n","    # Image Augmentation through preprocessing layer\n","    preprocessing = tf.keras.Sequential(\n","        [\n","            tfkl.RandomFlip(),\n","            tfkl.RandomRotation((-0.2, 0.2)),\n","            tfkl.RandomZoom(0.1),\n","            tfkl.RandomTranslation(0.2, 0.2)\n","        ],\n","        name='preprocessing'\n","    ) (input_layer)\n","    \n","    # RandAugment layer\n","    # rand_augment = keras_cv.layers.RandAugment(value_range=(0, 1), magnitude=0.2)(preprocessing)\n","       \n","    base_model = getBaseModel(preprocessing)\n","\n","    x = tfkl.Dense(\n","        units = 256,\n","        kernel_initializer = tfk.initializers.HeUniform(seed=SEED),\n","        name = \"HiddenDense1\",\n","    )(base_model)\n","    # Batch Normalization\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.Activation('swish')(x)\n","    \n","    # Dropout\n","    # x = tfkl.Dropout(DROPOUT_RATE, seed=SEED, name=\"Dropout1\")(x)\n","    \n","    \n","    x = tfkl.Dense(\n","        units = 128,\n","        kernel_initializer = tfk.initializers.HeUniform(seed=SEED),\n","        name = \"HiddenDense2\",\n","    )(x)\n","    # Batch Normalization\n","    x = tfkl.BatchNormalization()(x)\n","    x = tfkl.Activation('swish')(x) \n","\n","    # Dropout\n","    # x = tfkl.Dropout(DROPOUT_RATE, seed=SEED, name=\"Dropout2\")(x)\n","\n","    outputs = tfkl.Dense(\n","        2,\n","        kernel_initializer = tfk.initializers.GlorotUniform(seed=SEED),\n","        activation='softmax'\n","    )(x)\n","\n","    model = tfk.Model(input_layer, outputs, name='model')\n","        \n","    # Freeze the first N layers\n","    layers = model.get_layer(getLayerToFreeze()).layers\n","    N = ceil(len(layers) * 0.2)\n","    for _, layer in enumerate(layers[:N]):\n","        layer.trainable = False\n","    \n","    # Compile the model with Categorical Cross-Entropy loss and Adam optimizer\n","    model.compile(\n","        loss = tfk.losses.CategoricalCrossentropy(),\n","        # Optimizer that implements the AdamW algorithm\n","        optimizer = tfk.optimizers.AdamW(learning_rate=LR),\n","        metrics = ['accuracy']\n","    )\n","    return model"]},{"cell_type":"markdown","metadata":{},"source":["## Model Fitting"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:46.120477Z","iopub.status.busy":"2023-11-12T22:24:46.120238Z","iopub.status.idle":"2023-11-12T22:24:46.130553Z","shell.execute_reply":"2023-11-12T22:24:46.129783Z","shell.execute_reply.started":"2023-11-12T22:24:46.120455Z"},"trusted":true},"outputs":[],"source":["def fitModel(X_train, y_train, X_val, y_val, use_datagen, epochs):\n","    model = build_model()\n","\n","    validation_data = (X_val*255, y_val) if not X_val is None else None\n","\n","    class_weights = compute_class_weights(labels, y_train)\n","\n","    # X_train_oversampled, y_train_oversampled = dataAugImbalanced(X_train, y_train)\n","\n","    # check whether use ImageDataGenerator or not\n","    if not use_datagen:\n","        model.fit(\n","            x = X_train*255,\n","            y = y_train,\n","            validation_data = validation_data,\n","            epochs = epochs,\n","            batch_size = BATCH_SIZE,\n","            class_weight = class_weights,\n","            callbacks = callbacks\n","        ).history\n","        \n","    else: \n","        model.fit_generator(\n","            dataAugmentation.flow(\n","                X_train*255,\n","                y_train,\n","                batch_size = BATCH_SIZE,\n","                shuffle = True\n","            ),\n","            validation_data = validation_data,\n","            epochs = epochs,\n","            class_weight = class_weights,\n","            steps_per_epoch = len(X_train)/BATCH_SIZE,\n","            callbacks = callbacks\n","        ).history    \n","    return model"]},{"cell_type":"markdown","metadata":{"id":"RZDHF8DGvG2d"},"source":["## K-Fold Cross-Validation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:46.131980Z","iopub.status.busy":"2023-11-12T22:24:46.131679Z","iopub.status.idle":"2023-11-12T22:24:46.143731Z","shell.execute_reply":"2023-11-12T22:24:46.142802Z","shell.execute_reply.started":"2023-11-12T22:24:46.131947Z"},"id":"w-FFJBnCvG2d","outputId":"efc0d714-202d-4d82-d332-1a2fe8ca3805","trusted":true},"outputs":[],"source":["def KFoldCrossValidation(X_train_val, y_train_val, use_datagen):\n","    # Define the number of folds for cross-validation\n","    num_folds = 5\n","\n","    # Initialize lists to store training histories, scores, and best epochs\n","    histories = []\n","    scores = []\n","    best_epochs = []\n","\n","    # Create a KFold cross-validation object\n","    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=SEED)\n","\n","    # Loop through each fold\n","    for fold_idx, (train_idx, valid_idx) in enumerate(kfold.split(X_train_val, y_train_val)):\n","        print(f\"Starting training on fold num: {fold_idx+1}\")\n","\n","        # Build a new model for each fold\n","        k_model = fitModel(X_train_val[train_idx], y_train_val[train_idx], X_train_val[valid_idx], y_train_val[train_idx], use_datagen=False, epochs=EPOCHS)\n","            \n","        # Evaluate the model on the validation data for this fold\n","        score = k_model.evaluate(X_train_val[valid_idx], y_train_val[valid_idx], verbose=0)\n","        scores.append(score[1])\n","\n","        # Calculate the best epoch for early stopping\n","        best_epoch = len(k_model.history['loss']) - ES_PATIENCE\n","        best_epochs.append(best_epoch)\n","\n","        # Store the training history for this fold\n","        histories.append(k_model.history)\n","\n","    # Calculate the average best epoch\n","    avg_epochs = int(np.mean(best_epochs))\n","    print(f\"Best average epoch: {avg_epochs}\")\n","    \n","    return fitModel(X_train_val, y_train_val, None, None, use_datagen=False, epochs=avg_epochs)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T22:24:46.145143Z","iopub.status.busy":"2023-11-12T22:24:46.144834Z","iopub.status.idle":"2023-11-12T23:53:55.106168Z","shell.execute_reply":"2023-11-12T23:53:55.105186Z","shell.execute_reply.started":"2023-11-12T22:24:46.145111Z"},"id":"KZ-I9iJXvG2d","outputId":"e90fb12a-e86b-4192-8a59-c53afc635fc8","trusted":true},"outputs":[],"source":["hypertuning = False\n","use_datagen = False\n","\n","model = fitModel(X_train, y_train, X_val, y_val, use_datagen, epochs=EPOCHS)\n","# model = KFoldCrossValidation(X_train_val, y_train_val, use_datagen)\n","# best_hp = start_hypertuning(X_train, y_train); hypertuning = True"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-12T23:53:55.107907Z","iopub.status.busy":"2023-11-12T23:53:55.107550Z","iopub.status.idle":"2023-11-12T23:59:34.300244Z","shell.execute_reply":"2023-11-12T23:59:34.299229Z","shell.execute_reply.started":"2023-11-12T23:53:55.107875Z"},"id":"q26GO3RTHiWY","trusted":true},"outputs":[],"source":["if not hypertuning:\n","    # Save the model\n","    model.save('model')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T00:03:30.882332Z","iopub.status.busy":"2023-11-13T00:03:30.881487Z"},"trusted":true},"outputs":[],"source":["if PLATFORM == \"Kaggle\":\n","    if not hypertuning:\n","        os.chdir(r'/kaggle/working')\n","        !zip -r file.zip /kaggle/working"]},{"cell_type":"markdown","metadata":{"id":"Ex4-d7UgvG2e"},"source":["## Test Time Augmentation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T00:00:43.541111Z","iopub.status.busy":"2023-11-13T00:00:43.540763Z","iopub.status.idle":"2023-11-13T00:01:37.425298Z","shell.execute_reply":"2023-11-13T00:01:37.424304Z","shell.execute_reply.started":"2023-11-13T00:00:43.541076Z"},"id":"5NwPsNcvvG2e","trusted":true},"outputs":[],"source":["if not hypertuning:\n","    tta_steps = 10\n","    predictions = []\n","\n","    for i in tqdm(range(tta_steps)):\n","        preds = model.predict_generator(\n","            dataAugmentation.flow(\n","                X_test,\n","                batch_size = BATCH_SIZE,\n","                shuffle = True\n","            ),\n","            steps=ceil(X_test.shape[0])\n","        )\n","        predictions.append(preds)\n","\n","    pred = np.mean(predictions, axis=0)\n","\n","    print(np.mean(np.equal(np.argmax(y_test, axis=-1), np.argmax(pred, axis=-1))))"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def predict(model, X_test, y_test):\n","    preprocess_input = getPreprocessToUse()\n","    \n","    # Predict labels for the entire test set\n","    predictions = model.predict(preprocess_input(X_test*255))\n","\n","    # Compute classification metrics\n","    accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n","    precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","    recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","    f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","\n","    # Display the computed metrics\n","    print('Accuracy:', accuracy.round(4))\n","    print('Precision:', precision.round(4))\n","    print('Recall:', recall.round(4))\n","    print('F1:', f1.round(4))\n","\n","    # Compute the confusion matrix\n","    cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), normalize=\"true\")\n","\n","    # Plot the confusion matrix\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm.T, xticklabels=list(('healthy','unhealthy')), yticklabels=list(('healthy','unhealthy')), cmap='Blues', annot=True)\n","    plt.xlabel('True labels')\n","    plt.ylabel('Predicted labels')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-11-13T00:01:37.427153Z","iopub.status.busy":"2023-11-13T00:01:37.426786Z","iopub.status.idle":"2023-11-13T00:01:41.512847Z","shell.execute_reply":"2023-11-13T00:01:41.511804Z","shell.execute_reply.started":"2023-11-13T00:01:37.427120Z"},"id":"5utKmgTXnf_N","jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["if not hypertuning:\n","    predict(model, X_test, y_test)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":3989928,"sourceId":6947272,"sourceType":"datasetVersion"}],"dockerImageVersionId":30580,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
